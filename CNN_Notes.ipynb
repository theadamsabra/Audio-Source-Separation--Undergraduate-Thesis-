{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzivc4MzP+H2YKSgGg2L1P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theadamsabra/Audio-Source-Separation-Undergraduate-Thesis/blob/master/CNN_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIx-H3q6k3p4",
        "colab_type": "text"
      },
      "source": [
        "# Data Driven Approach:\n",
        "\n",
        "1. Collect dataset of images + labels\n",
        "2. Use ML to train a classifier\n",
        "3. Test model on new images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxggsAdNk3ys",
        "colab_type": "text"
      },
      "source": [
        "## Nearest Neighbor:\n",
        "\n",
        "- Training: memorizes all data and labels\n",
        "- Testing: predict the label of most similar training image\n",
        "\n",
        "How do we mathematically define similar?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh2ZC0l0k36L",
        "colab_type": "text"
      },
      "source": [
        "## Distance Metrics:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBkXzxR0oBVx",
        "colab_type": "text"
      },
      "source": [
        "### L1 Distance (Manhatten):\n",
        "\n",
        "$$d(I_1 - I_2) = \\displaystyle\\sum_p | I_1^p - I_2^p | $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuYNTIHVnwEM",
        "colab_type": "text"
      },
      "source": [
        "## Problems with Data Driven Approach:\n",
        "\n",
        "Training is $O(1)$ time whereas testing time is $O(n)$. This is not useful as we want most models to test on new situations in real time. Ideal models would have opposite times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_Im08fb-UZ8",
        "colab_type": "text"
      },
      "source": [
        "# Linear Classifier:\n",
        "\n",
        "$$f(x, W) = Wx + b$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-WNaYtPAYJI",
        "colab_type": "text"
      },
      "source": [
        "## Loss Functions:\n",
        "\n",
        "Given a dataset $\\{ (x_i, y_i) \\}_{i=1}^N$ where $x_i$ is the image and $y_i$ is an integer representing the label.\n",
        "\n",
        "Loss is mathematically defined as the sum of loss over examples:\n",
        "$$L = \\frac{1}{N} \\displaystyle\\sum_i L_i(f(x_i, W), y_i) $$\n",
        "\n",
        "$L_i$ can be chosen arbitrarily depending on the problem. Below are some $L_i$ functions that can be uesd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMA7eXIqE7le",
        "colab_type": "text"
      },
      "source": [
        "### Multiclass SVM Classifier Loss:\n",
        "$$L_i = \\displaystyle\\sum_{j \\neq y} \\max(0, s_j - s_{y_i} + \\Delta)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMuok-iv_V_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def LiMulticlassSVM(x,y,W,delta): # Need to double check this.\n",
        "  scores = W.dot(x)\n",
        "  L_i = np.sum(np.max(0, sj - y + delta))\n",
        "  L = (1 / x.shape[0]) * np.sum(L_i)\n",
        "  return L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz_WGjNpH2x3",
        "colab_type": "text"
      },
      "source": [
        "### Softmax Classifier Loss:\n",
        "\n",
        "$$L_i = -\\log \\left( \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} \\right) $$\n",
        "\n",
        "From a probabalistic viewpoint:\n",
        "\n",
        "$$ P(y_i | x_i, W) = \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1rNpaxe2KCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def p(x,y,W):\n",
        "  f = W.dot(x)\n",
        "  f -= np.max(f)\n",
        "  p = np.exp(f) / np.sum(np.exp(f))\n",
        "  return p\n",
        "\n",
        "def LiSoftmax(p):\n",
        "  return -np.log(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUY37rZk0EvL",
        "colab_type": "text"
      },
      "source": [
        "## Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5HZZ69U0Zxo",
        "colab_type": "text"
      },
      "source": [
        "The **gradient** is the vector of partial derivatives along each dimension. The slope in any direction is the **dot product** of the direction with the gradient. The direction of steepest descent is the negative gradient.\n",
        "\n",
        "- Numerical Gradient: approximate, slow, and easy to write.\n",
        "- Analytic gradient: exact, fast, error-prone\n",
        "\n",
        "We always use analytic gradient but we can check our implementation with the numerical gradient. This is called a gradient check. It makes a great debugging tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI_RIXEA2UzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Psuedocode for vanilla gradient descent\n",
        "\n",
        "'''\n",
        "while True:\n",
        "  weights_gradient = evaluate_gradient(loss_fun, data, weights)\n",
        "  weights += - step_size * weights_grad \n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNcAUT2U3sTJ",
        "colab_type": "text"
      },
      "source": [
        "### Stochastic Gradient Descent (SGD)\n",
        "\n",
        "$$\\Delta_W L(W) = \\frac{1}{N} \\displaystyle\\sum_{i=1}^N \\Delta_W L_i(x_i, y_i, W) + \\lambda \\Delta_W R(W) $$\n",
        "\n",
        "where $N$ is a **minibatch** of examples (usally a power of 2) from the dataset. We evaluate the gradient of the weights in each batch and update the weights from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1HRCzn33Iil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Psuedocode for vanilla minibatch gradient descent\n",
        "\n",
        "'''\n",
        "while True:\n",
        "  minibatch = sample_traning_data(data, 256)\n",
        "  weights_gradient = evaluate_gradient(loss_function, minibatch, weights)\n",
        "  weights += - step_size * weights_grad \n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}