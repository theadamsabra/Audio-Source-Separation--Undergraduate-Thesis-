{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "\n",
    "- [x] Import hdf5 file as np.array\n",
    "\n",
    "[ ] Build rough draft of U-Net model\n",
    "\n",
    "[ ] Make code more reusable/object oriented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 to Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, ReLU, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `librosa` shapes the output arrays slightly differently than we want. `128` represents the number of mel coefficients we have $\\implies$ that should be in `array.shape[2]` rather than where it is now in `array.shape[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(path, key):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        array = np.array(f[key], dtype = 'float64')\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '/home/asabra/GitHub/Audio-Source-Separation-Undergraduate-Thesis/data/Test.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-242dc588424f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/asabra/GitHub/Audio-Source-Separation-Undergraduate-Thesis/data/Test.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load in hdf5 datasets as np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmixture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixture'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-427d89291b84>\u001b[0m in \u001b[0;36mloader\u001b[0;34m(path, key)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.8/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                                swmr=swmr)\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.8/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/home/asabra/GitHub/Audio-Source-Separation-Undergraduate-Thesis/data/Test.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "path = '/home/asabra/GitHub/Audio-Source-Separation-Undergraduate-Thesis/data/Test.hdf5'\n",
    "# Load in hdf5 datasets as np.array\n",
    "mixture = loader(path, 'mixture')\n",
    "target = loader(path, 'target')\n",
    "\n",
    "print(f'Mixture Shape: {mixture.shape} \\n Target Shape: {target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be \"training\" on a subset of the training to verify that the model actually works. Once complete, we will run it on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINGING VOICE SEPARATION WITH DEEP U-NET CONVOLUTIONAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Our implementation of U-Net is similar to that of [11].\n",
    "Each encoder layer consists of a strided 2D convolution\n",
    "of stride 2 and kernel size 5x5, batch normalization, and\n",
    "leaky rectified linear units (ReLU) with leakiness 0.2. In\n",
    "the decoder we use strided deconvolution (sometimes re-\n",
    "ferred to as transposed convolution) with stride 2 and ker-\n",
    "nel size 5x5, batch normalization, plain ReLU, and use\n",
    "50% dropout to the first three layers, as in [11]. In the final\n",
    "layer we use a sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmix = mixture[:5, :, :]\n",
    "testvoc = target[:5, :, :]\n",
    "testmix = testmix.reshape(testmix.shape[0], testmix.shape[2],\n",
    "                         testmix.shape[1], 1) # Make sure it's 4D\n",
    "testvoc = testvoc.reshape(testvoc.shape[0], testvoc.shape[2],\n",
    "                         testvoc.shape[1], 1) # Same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on 5 to ensure code scales.\n",
    "print(f'Mixture Shape:{testmix.shape} \\n Target Shape: {testvoc.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tf.keras.layers.Conv2D(\n",
    "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
    "    dilation_rate=(1, 1), groups=1, activation=None, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
    ")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel_size\n",
    "- strides\n",
    "- padding\n",
    "- activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Example of How Bad Code Helps You Think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_size = (5,5)\n",
    "# strides = (2,2)\n",
    "# leaky_alpha = 0.2\n",
    "\n",
    "# # Initialize Model\n",
    "# model = Sequential()\n",
    "# # First Convolution\n",
    "# model.add(Conv2D(16, kernel_size, strides))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(LeakyReLU(leaky_alpha))\n",
    "# # Second Convolution\n",
    "# model.add(Conv2D(32))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(LeakyReLU(leaky_alpha))\n",
    "# # Third Convolution\n",
    "# model.add(Conv2D(64))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(LeakyReLU(leaky_alpha))\n",
    "# # Fourth Convolution\n",
    "# model.add(Conv2D(128))\n",
    "# # Fifth Convolution\n",
    "# model.add(Conv2D(256))\n",
    "# # Final Convolution\n",
    "# model.add(Conv2D(512))\n",
    "# model.add(Conv2DTranspose(256))\n",
    "# model.add(Conv2DTranspose(128))\n",
    "# model.add(Conv2DTranspose(64))\n",
    "# model.add(Conv2DTranspose(32))\n",
    "# model.add(Conv2DTranspose(16))\n",
    "# # \"flatten\" or conv2dtranspose(1) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hdf5 in data folder\n",
    "data_folder_path = '/home/asabra/Github/Audio-Source-Separation-Undergraduate-Thesis-/data'\n",
    "for dev_test in ['Dev', 'Test']:\n",
    "    true_path = os.path.join(data_folder_path, dev_test + '.hdf5')\n",
    "    with h5py.File(true_path, 'r') as f:\n",
    "        if dev_test == 'Dev':\n",
    "            test = tf.convert_to_tensor(f[0:10,:,:], dtype = 'float64')\n",
    "        else:\n",
    "            train = tf.convert_to_tensor(f[0:10,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nicer looking code\n",
    "class VocalUNet(tf.keras.Model):\n",
    "    def __init__(self, inputs, kernel_size, strides):\n",
    "        super(VocalUNet, self).__init__()\n",
    "        self.conv1 = BatchNormalization(Conv2D(inputs = inputs,\n",
    "        filters = 16, kernel_size = kernel_size, strides = strides,\n",
    "        activation = leaky_relu))\n",
    "        self.conv2 = BatchNormalization(Conv2D(inputs = inputs,\n",
    "        filters = 32, kernel_size = kernel_size, strides = strides,\n",
    "        activation = leaky_relu))\n",
    "        self.conv3 = BatchNormalization(Conv2D(inputs = inputs,\n",
    "        filters = 64, kernel_size = kernel_size, strides = strides,\n",
    "        activation = leaky_relu)) \n",
    "        self.conv4 = BatchNormalization(Conv2D(inputs = inputs,\n",
    "        filters = 128, kernel_size = kernel_size, strides = strides,\n",
    "        activation = leaky_relu)) # Sort of\n",
    "        self.conv5 = BatchNormalization(Conv2D(inputs = inputs,\n",
    "        filters = 256, kernel_size = kernel_size, strides = strides,\n",
    "        activation = leaky_relu))\n",
    "        self.conv6 = BatchNormalization(Conv2D(inputs = inputs,\n",
    "        filters = 512, kernel_size = kernel_size, strides = strides,\n",
    "        activation = leaky_relu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
